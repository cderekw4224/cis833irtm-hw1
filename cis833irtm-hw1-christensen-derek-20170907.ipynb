{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Descriptive text analytics, first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk, which is a python package for natural language processing\n",
    "import nltk\n",
    "# to remove stropwords\n",
    "from nltk.corpus import stopwords\n",
    "# FreqDist, word_tokenize\n",
    "from nltk import FreqDist, sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# to read and/or save text files or csv files, import csv\n",
    "import csv\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# python package for text classification\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#initialize countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#initialize TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# support vector machine (another algorithm for classification)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluating model performance\n",
    "from sklearn import metrics\n",
    "\n",
    "# Excel-like format\n",
    "import pandas as pd\n",
    "# to diplay max rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# to diplay max cols\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "# to define width of cells\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "# package for numbers\n",
    "import numpy as np\n",
    "\n",
    "# Regular Expression\n",
    "import re\n",
    "\n",
    "# WordCloud\n",
    "from os import path\n",
    "#####from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Count words in list\n",
    "from collections import Counter\n",
    "\n",
    "# Pattern\n",
    "#####from pattern.en import sentiment\n",
    "\n",
    "# Seaborn\n",
    "import seaborn\n",
    "\n",
    "# IPyton Display\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# You can include Youtube video in Ipython notebook\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Include images in ipython notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# Include webpages in ipython notebook\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from operator import itemgetter, attrgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review = []\n",
    "\n",
    "##NB: ##--double hash-tags & dashes denote comments in my original code\n",
    "##  : # single hash-tags denote my original code\n",
    "\n",
    "##---------------------------------\n",
    "##-- My original file handling code to read in one file\n",
    "\n",
    "# openfile = open('data/abdennadher00experimental', 'rb')\n",
    "\n",
    "##--openfile = open('data/*.txt', 'rb')\n",
    "\n",
    "# r = csv.reader(openfile)\n",
    "# for i in r:\n",
    "     ##-- get the first column only (ignoring the second column)\n",
    "#    review.append(i)    \n",
    "# openfile.close()\n",
    "\n",
    "############################################################\n",
    "# path to citeseer directory\n",
    "#dir_path = ''./citeseer\"\n",
    "\n",
    "#dir_path = ''./data\"  # didn't work\n",
    "#dir_path = ''/data\"  # didn't work\n",
    "#dir_path = ''C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data\"    # didn't work\n",
    "#dir_path = ''./Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data\"    # didn't work\n",
    "#dir_path = ''./Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data\"   # didn't work\n",
    "\n",
    "dir_path = 'C:/Users/Derek Christensen/Dropbox/_cis833irtm/hw1/data'\n",
    "\n",
    "# get all files inside the directory\n",
    "files = os.listdir(dir_path)\n",
    "    \n",
    "# tokenize the words based on white space, removes the punctuation\n",
    "for f in files:\n",
    "    ipfile = open(dir_path+'/'+os.path.basename(f),'r')\n",
    "    for i in ipfile:\n",
    "        review.append(i)\n",
    "\n",
    "    # start processing the ipfile\n",
    "#############################################################\n",
    "\n",
    "#print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Facilitating Message Exchange though Middle Agents To utilize services provided by other agents, a requesting agent needs to locate and communicate with these service providers. Specifically, in order to interoperate with the providers, the requesting agent should know: 1) the service provider's interface; 2) the ontology that defines concepts used by the provider; and 3) the agent communication language (ACL) the agent uses so that it can parse and understand the communication. Currently deployed Multi-Agent Systems (MAS) encode the interface description and the ontology within a service provider's capability description (or advertisement) that is registered with a Middle Agent; however, this assumes a common ACL between communicating agents. We demonstrate how agents can communicate with each other using a template-based shallow parsing approach to constructing and decomposing messages, thus relaxing assumptions on the ACLs and message formats used.\\n\", 'Practical Guidelines for the Readability of IT-architecture Diagrams This paper presents the work done to establish guidelines for the creation of readable IT-architecture diagrams and gives some examples of guidelines and some examples of improved diagrams. These guidelines are meant to assist practicing IT-architects in preparing the diagrams to communicate their architectures to the various stakeholders. Diagramming has always been important in information technology (IT), but the recent interest in ITarchitecture, the widespread use of software and developments in electronic communication, make it necessary to again look at the rt of making diagrams\\'for this particular class and its users. The guidelines indicate how various visual attributes, like hierarchy, layout, color, form, graphics, etc. can contribute to the readability of IT-architecture diagrams. The emphasis is on the outward appearance of diagrams. Some additional support is given for the thinking/reasoning processes while designing or using a set of diagrams and an attempt is made to arrive at a rationale of these guidelines. An evaluation process has been performed with three groups of practicing IT-architects. The outcome of this evaluation is presented. This work is part of a more comprehensive research project on \"Visualisation of IT- architecture\".\\n']\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 words in list review\n",
    "print(review[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# currently the data are in list ... convert to string. \n",
    "# even though there are many files, we will treat it as a single large document\n",
    "tokens = str(review)\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Facilitating Message Exchange though Middle Agents To utilize services provided by other agents, a\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowecases for content analytics ... we assume, for example, LOVE is sames love \n",
    "tokens = tokens.lower()\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"facilitating message exchange though middle agents to utilize services provided by other agents, a\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the dataset contains useless characters and numbers\n",
    "# Remove useless numbers and alphanumerical words\n",
    "# use regular expression ... a-zA-Z0-9 refers to all English characters (lowercase & uppercase) and numbers\n",
    "# ^a-zA-Z0-9 is opposite of a-zA-Z0-9\n",
    "tokens = re.sub(\"[^a-zA-Z0-9]\", \" \", tokens)\n",
    "#print(tokens)\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  facilitating message exchange though middle agents to utilize services provided by other agents  a\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 characters in list tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenization or word split\n",
    "tokens = word_tokenize(tokens)\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facilitating', 'message', 'exchange', 'though', 'middle', 'agents', 'to', 'utilize', 'services', 'provided', 'by', 'other', 'agents', 'a', 'requesting', 'agent', 'needs', 'to', 'locate', 'and']\n"
     ]
    }
   ],
   "source": [
    "# print 1st 20 tokens in list tokens\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n"
     ]
    }
   ],
   "source": [
    "totwords = len(tokens)\n",
    "print(totwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'03': 1,\n",
       "          '1': 4,\n",
       "          '10': 1,\n",
       "          '14': 1,\n",
       "          '1998': 1,\n",
       "          '2': 2,\n",
       "          '2000': 1,\n",
       "          '2003': 1,\n",
       "          '3': 2,\n",
       "          'a': 28,\n",
       "          'abbot': 1,\n",
       "          'abduction': 3,\n",
       "          'abecker': 2,\n",
       "          'above': 2,\n",
       "          'abstract': 1,\n",
       "          'access': 2,\n",
       "          'achieve': 1,\n",
       "          'acl': 2,\n",
       "          'acls': 1,\n",
       "          'acm': 1,\n",
       "          'acoustic': 1,\n",
       "          'active': 1,\n",
       "          'additional': 1,\n",
       "          'adds': 1,\n",
       "          'advantage': 1,\n",
       "          'advertisement': 1,\n",
       "          'again': 1,\n",
       "          'agent': 6,\n",
       "          'agents': 4,\n",
       "          'aggregation': 1,\n",
       "          'al': 2,\n",
       "          'algorithms': 3,\n",
       "          'almost': 2,\n",
       "          'also': 1,\n",
       "          'always': 1,\n",
       "          'american': 2,\n",
       "          'an': 5,\n",
       "          'and': 33,\n",
       "          'answering': 1,\n",
       "          'appear': 2,\n",
       "          'appearance': 1,\n",
       "          'applications': 2,\n",
       "          'applied': 2,\n",
       "          'approach': 2,\n",
       "          'approaches': 2,\n",
       "          'approximation': 1,\n",
       "          'architects': 2,\n",
       "          'architecture': 4,\n",
       "          'architectures': 1,\n",
       "          'archive': 1,\n",
       "          'archives': 1,\n",
       "          'are': 7,\n",
       "          'area': 1,\n",
       "          'around': 1,\n",
       "          'arrive': 1,\n",
       "          'as': 4,\n",
       "          'aspects': 1,\n",
       "          'assist': 1,\n",
       "          'assumes': 1,\n",
       "          'assumptions': 1,\n",
       "          'at': 2,\n",
       "          'attempt': 1,\n",
       "          'attributes': 1,\n",
       "          'automatic': 1,\n",
       "          'based': 4,\n",
       "          'basic': 1,\n",
       "          'be': 3,\n",
       "          'beds': 1,\n",
       "          'been': 2,\n",
       "          'being': 1,\n",
       "          'below': 2,\n",
       "          'besides': 1,\n",
       "          'between': 3,\n",
       "          'bottom': 1,\n",
       "          'bound': 1,\n",
       "          'bounded': 1,\n",
       "          'bpokm': 2,\n",
       "          'british': 2,\n",
       "          'broadcast': 5,\n",
       "          'build': 1,\n",
       "          'builds': 1,\n",
       "          'business': 2,\n",
       "          'but': 2,\n",
       "          'by': 4,\n",
       "          'can': 5,\n",
       "          'capability': 1,\n",
       "          'cases': 1,\n",
       "          'characterization': 1,\n",
       "          'chr': 2,\n",
       "          'class': 3,\n",
       "          'clp': 5,\n",
       "          'coincidence': 1,\n",
       "          'collaboration': 1,\n",
       "          'color': 1,\n",
       "          'combinations': 1,\n",
       "          'common': 1,\n",
       "          'communicate': 3,\n",
       "          'communicating': 1,\n",
       "          'communication': 3,\n",
       "          'comparing': 1,\n",
       "          'comprehensive': 1,\n",
       "          'computational': 1,\n",
       "          'concepts': 1,\n",
       "          'concerned': 1,\n",
       "          'confirms': 1,\n",
       "          'consolidates': 1,\n",
       "          'constant': 3,\n",
       "          'constraint': 5,\n",
       "          'constraints': 1,\n",
       "          'constructing': 1,\n",
       "          'construction': 1,\n",
       "          'context': 2,\n",
       "          'contribute': 1,\n",
       "          'creation': 1,\n",
       "          'currently': 1,\n",
       "          'data': 1,\n",
       "          'databases': 3,\n",
       "          'decomposing': 1,\n",
       "          'decor': 4,\n",
       "          'define': 1,\n",
       "          'defines': 1,\n",
       "          'defini': 1,\n",
       "          'delivery': 2,\n",
       "          'demonstrate': 1,\n",
       "          'deployed': 1,\n",
       "          'describe': 1,\n",
       "          'described': 1,\n",
       "          'description': 2,\n",
       "          'designing': 1,\n",
       "          'detailed': 1,\n",
       "          'development': 1,\n",
       "          'developments': 1,\n",
       "          'develops': 1,\n",
       "          'diagramming': 1,\n",
       "          'diagrams': 8,\n",
       "          'dis': 1,\n",
       "          'discrete': 2,\n",
       "          'discuss': 1,\n",
       "          'disjunctive': 1,\n",
       "          'distance': 6,\n",
       "          'dkns01': 1,\n",
       "          'document': 2,\n",
       "          'done': 1,\n",
       "          'down': 1,\n",
       "          'each': 3,\n",
       "          'earlier': 1,\n",
       "          'effect': 1,\n",
       "          'efficient': 1,\n",
       "          'electronic': 1,\n",
       "          'embedded': 1,\n",
       "          'emphasis': 1,\n",
       "          'encode': 1,\n",
       "          'end': 1,\n",
       "          'english': 1,\n",
       "          'enhanced': 1,\n",
       "          'environments': 1,\n",
       "          'equivalence': 1,\n",
       "          'equivalent': 1,\n",
       "          'esprit': 1,\n",
       "          'establish': 1,\n",
       "          'established': 1,\n",
       "          'et': 2,\n",
       "          'etc': 1,\n",
       "          'evaluation': 7,\n",
       "          'examples': 2,\n",
       "          'exchange': 1,\n",
       "          'executed': 1,\n",
       "          'existing': 1,\n",
       "          'expansion': 1,\n",
       "          'experimental': 1,\n",
       "          'experimenting': 1,\n",
       "          'expressibility': 1,\n",
       "          'expressing': 1,\n",
       "          'extended': 1,\n",
       "          'extension': 1,\n",
       "          'facilitate': 1,\n",
       "          'facilitating': 1,\n",
       "          'factor': 1,\n",
       "          'field': 1,\n",
       "          'following': 1,\n",
       "          'for': 18,\n",
       "          'form': 1,\n",
       "          'formal': 1,\n",
       "          'formally': 1,\n",
       "          'formats': 1,\n",
       "          'framework': 1,\n",
       "          'from': 3,\n",
       "          'given': 3,\n",
       "          'gives': 1,\n",
       "          'goal': 1,\n",
       "          'good': 2,\n",
       "          'graphics': 1,\n",
       "          'groups': 1,\n",
       "          'guidelines': 6,\n",
       "          'has': 4,\n",
       "          'helpful': 1,\n",
       "          'hence': 1,\n",
       "          'here': 1,\n",
       "          'hierarchy': 1,\n",
       "          'higher': 1,\n",
       "          'how': 2,\n",
       "          'however': 1,\n",
       "          'i': 1,\n",
       "          'idea': 1,\n",
       "          'identical': 1,\n",
       "          'identify': 1,\n",
       "          'identifying': 1,\n",
       "          'if': 1,\n",
       "          'ii': 1,\n",
       "          'imply': 1,\n",
       "          'important': 2,\n",
       "          'improved': 1,\n",
       "          'improvement': 1,\n",
       "          'in': 17,\n",
       "          'indexing': 1,\n",
       "          'indicate': 1,\n",
       "          'inequality': 1,\n",
       "          'information': 3,\n",
       "          'innovative': 1,\n",
       "          'integration': 1,\n",
       "          'integrity': 3,\n",
       "          'intended': 1,\n",
       "          'intensive': 1,\n",
       "          'interest': 1,\n",
       "          'interface': 2,\n",
       "          'interoperate': 1,\n",
       "          'into': 1,\n",
       "          'introduce': 1,\n",
       "          'introduction': 2,\n",
       "          'is': 14,\n",
       "          'it': 11,\n",
       "          'itarchitecture': 1,\n",
       "          'iterative': 1,\n",
       "          'its': 2,\n",
       "          'j': 1,\n",
       "          'k': 3,\n",
       "          'know': 1,\n",
       "          'knowledge': 6,\n",
       "          'knowmore': 1,\n",
       "          'language': 2,\n",
       "          'languages': 1,\n",
       "          'large': 3,\n",
       "          'latter': 1,\n",
       "          'layout': 1,\n",
       "          'like': 1,\n",
       "          'limitations': 1,\n",
       "          'lists': 3,\n",
       "          'locate': 1,\n",
       "          'logic': 2,\n",
       "          'long': 1,\n",
       "          'look': 1,\n",
       "          'made': 1,\n",
       "          'make': 1,\n",
       "          'making': 1,\n",
       "          'management': 1,\n",
       "          'manner': 1,\n",
       "          'mas': 1,\n",
       "          'mathematics': 1,\n",
       "          'meant': 1,\n",
       "          'measure': 1,\n",
       "          'measures': 6,\n",
       "          'memories': 1,\n",
       "          'memory': 1,\n",
       "          'message': 2,\n",
       "          'messages': 1,\n",
       "          'methodology': 1,\n",
       "          'methods': 1,\n",
       "          'metric': 3,\n",
       "          'metrics': 1,\n",
       "          'middle': 2,\n",
       "          'model': 2,\n",
       "          'more': 2,\n",
       "          'motivate': 1,\n",
       "          'motivated': 1,\n",
       "          'multi': 1,\n",
       "          'multimedia': 1,\n",
       "          'multiparadigm': 1,\n",
       "          'multiples': 2,\n",
       "          'n': 6,\n",
       "          'navigation': 1,\n",
       "          'necessary': 1,\n",
       "          'needs': 1,\n",
       "          'network': 1,\n",
       "          'new': 1,\n",
       "          'news': 5,\n",
       "          'north': 2,\n",
       "          'not': 2,\n",
       "          'notion': 1,\n",
       "          'notions': 2,\n",
       "          'obj': 1,\n",
       "          'of': 39,\n",
       "          'offer': 1,\n",
       "          'om': 1,\n",
       "          'on': 8,\n",
       "          'ontology': 2,\n",
       "          'or': 2,\n",
       "          'order': 1,\n",
       "          'organisational': 1,\n",
       "          'organised': 1,\n",
       "          'organizational': 2,\n",
       "          'oriented': 1,\n",
       "          'originally': 1,\n",
       "          'other': 3,\n",
       "          'others': 1,\n",
       "          'our': 2,\n",
       "          'outcome': 1,\n",
       "          'outward': 1,\n",
       "          'overcome': 1,\n",
       "          'paper': 3,\n",
       "          'parse': 1,\n",
       "          'parsing': 1,\n",
       "          'part': 1,\n",
       "          'particular': 2,\n",
       "          'performed': 2,\n",
       "          'performs': 1,\n",
       "          'platform': 2,\n",
       "          'polygonal': 1,\n",
       "          'polynomial': 1,\n",
       "          'positive': 1,\n",
       "          'practical': 1,\n",
       "          'practicing': 2,\n",
       "          'preparing': 1,\n",
       "          'present': 1,\n",
       "          'presented': 1,\n",
       "          'presents': 1,\n",
       "          'principal': 1,\n",
       "          'probabilistic': 1,\n",
       "          'problem': 1,\n",
       "          'process': 2,\n",
       "          'processes': 2,\n",
       "          'processing': 1,\n",
       "          'produce': 1,\n",
       "          'programmes': 1,\n",
       "          'programming': 2,\n",
       "          'project': 4,\n",
       "          'prolog': 1,\n",
       "          'provided': 1,\n",
       "          'provider': 3,\n",
       "          'providers': 2,\n",
       "          'query': 4,\n",
       "          'radio': 1,\n",
       "          'rank': 1,\n",
       "          'rationale': 1,\n",
       "          'readability': 2,\n",
       "          'readable': 1,\n",
       "          'realtime': 1,\n",
       "          'reasoning': 1,\n",
       "          'recent': 1,\n",
       "          'recognition': 1,\n",
       "          'recognizer': 1,\n",
       "          'recurrent': 1,\n",
       "          'reference': 1,\n",
       "          'registered': 1,\n",
       "          'relaxed': 1,\n",
       "          'relaxing': 1,\n",
       "          'report': 1,\n",
       "          'representations': 1,\n",
       "          'requesting': 2,\n",
       "          'research': 2,\n",
       "          'respect': 1,\n",
       "          'results': 3,\n",
       "          'retrieval': 5,\n",
       "          'rise': 1,\n",
       "          'robust': 1,\n",
       "          'rt': 1,\n",
       "          's': 2,\n",
       "          'satisfy': 1,\n",
       "          'sdr': 1,\n",
       "          'second': 1,\n",
       "          'seemingly': 1,\n",
       "          'segmentation': 1,\n",
       "          'sensitive': 2,\n",
       "          'serve': 1,\n",
       "          'service': 3,\n",
       "          'services': 2,\n",
       "          'set': 1,\n",
       "          'several': 1,\n",
       "          'shallow': 1,\n",
       "          'shortly': 1,\n",
       "          'should': 1,\n",
       "          'show': 2,\n",
       "          'shown': 1,\n",
       "          'siam': 2,\n",
       "          'similar': 1,\n",
       "          'similarity': 1,\n",
       "          'simple': 1,\n",
       "          'sketch': 1,\n",
       "          'so': 1,\n",
       "          'soda': 1,\n",
       "          'software': 1,\n",
       "          'solvers': 1,\n",
       "          'some': 6,\n",
       "          'specifically': 1,\n",
       "          'speech': 3,\n",
       "          'spoken': 2,\n",
       "          'stakeholders': 1,\n",
       "          'straightforward': 1,\n",
       "          'strikingly': 1,\n",
       "          'subset': 2,\n",
       "          'suggest': 1,\n",
       "          'support': 2,\n",
       "          'symposium': 1,\n",
       "          'system': 8,\n",
       "          'systems': 2,\n",
       "          'take': 1,\n",
       "          'task': 1,\n",
       "          'techniques': 1,\n",
       "          'technology': 2,\n",
       "          'television': 1,\n",
       "          'template': 1,\n",
       "          'term': 1,\n",
       "          'test': 1,\n",
       "          'tests': 1,\n",
       "          'text': 1,\n",
       "          'that': 8,\n",
       "          'the': 52,\n",
       "          'their': 1,\n",
       "          'then': 1,\n",
       "          'there': 1,\n",
       "          'thereby': 1,\n",
       "          'these': 6,\n",
       "          'they': 3,\n",
       "          'thinking': 1,\n",
       "          'this': 12,\n",
       "          'thisl': 3,\n",
       "          'though': 1,\n",
       "          'three': 2,\n",
       "          'thus': 1,\n",
       "          'time': 1,\n",
       "          'to': 26,\n",
       "          'together': 1,\n",
       "          'tool': 1,\n",
       "          'toolbox': 1,\n",
       "          'tools': 1,\n",
       "          'top': 4,\n",
       "          'trec': 1,\n",
       "          'triangle': 1,\n",
       "          'tries': 1,\n",
       "          'two': 5,\n",
       "          'understand': 1,\n",
       "          'unrelated': 1,\n",
       "          'up': 1,\n",
       "          'upon': 1,\n",
       "          'use': 1,\n",
       "          'used': 2,\n",
       "          'user': 2,\n",
       "          'users': 1,\n",
       "          'uses': 1,\n",
       "          'using': 5,\n",
       "          'utilize': 1,\n",
       "          'validation': 1,\n",
       "          'various': 3,\n",
       "          'version': 1,\n",
       "          'visual': 1,\n",
       "          'visualisation': 1,\n",
       "          'vocabulary': 1,\n",
       "          'we': 11,\n",
       "          'weaklystructured': 1,\n",
       "          'which': 4,\n",
       "          'while': 2,\n",
       "          'widespread': 1,\n",
       "          'with': 12,\n",
       "          'within': 1,\n",
       "          'work': 3,\n",
       "          'workflow': 2,\n",
       "          'writing': 1})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute frequency distribution for all the tokens in the text\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n"
     ]
    }
   ],
   "source": [
    "print(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 52), ('of', 39), ('and', 33), ('a', 28), ('to', 26), ('for', 18), ('in', 17), ('is', 14), ('this', 12), ('with', 12), ('we', 11), ('it', 11), ('diagrams', 8), ('system', 8), ('that', 8), ('on', 8), ('evaluation', 7), ('are', 7), ('guidelines', 6), ('agent', 6)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words in the Top 20:\n",
    "- the\n",
    "- of\n",
    "- and\n",
    "- a\n",
    "- to\n",
    "- for\n",
    "- in\n",
    "- is\n",
    "- this\n",
    "- with\n",
    "- we\n",
    "- it\n",
    "- that\n",
    "- on\n",
    "- are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.65\n"
     ]
    }
   ],
   "source": [
    "totwords15 = totwords * 0.15\n",
    "print(totwords15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>of</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>and</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>a</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>vocabulary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>weaklystructured</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>widespread</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>within</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>writing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0   1\n",
       "22                the  52\n",
       "396                of  39\n",
       "204               and  33\n",
       "109                 a  28\n",
       "11                 to  26\n",
       "..                ...  ..\n",
       "455        vocabulary   1\n",
       "145  weaklystructured   1\n",
       "98         widespread   1\n",
       "406            within   1\n",
       "120           writing   1\n",
       "\n",
       "[466 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the results of word frequency on corpus data as a list\n",
    "\n",
    "#freq_word_auto = []\n",
    "freq_word = []\n",
    "\n",
    "# two values or columns in fdist_a\n",
    "for k,v in fdist.items():\n",
    "    freq_word.append([k,v])\n",
    "\n",
    "#make it like an Excel worksheet\n",
    "wordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterative\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0  1\n",
      "0   iterative  1\n",
      "1  similarity  1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[:2][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterative\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0  1\n",
      "0     iterative  1\n",
      "1    similarity  1\n",
      "2       results  3\n",
      "3      existing  1\n",
      "4         query  4\n",
      "5     integrity  3\n",
      "6     reasoning  1\n",
      "7  consolidates  1\n",
      "8     concerned  1\n",
      "9   equivalence  1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[:10][:10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use `loc[]` to select a column\n",
    "print(df.loc[:,'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0  1\n",
      "0     iterative  1\n",
      "1    similarity  1\n",
      "2       results  3\n",
      "3      existing  1\n",
      "4         query  4\n",
      "..          ... ..\n",
      "461        beds  1\n",
      "462        2003  1\n",
      "463        2000  1\n",
      "464        time  1\n",
      "465  validation  1\n",
      "\n",
      "[466 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    iterative\n",
      "1            1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0  1\n",
      "0   iterative  1\n",
      "1  similarity  1\n",
      "2     results  3\n",
      "3    existing  1\n",
      "4       query  4\n",
      "5   integrity  3\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[0:5,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0  1\n",
      "0   iterative  1\n",
      "1  similarity  1\n",
      "2     results  3\n",
      "3    existing  1\n",
      "4       query  4\n",
      "5   integrity  3\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    iterative\n",
      "1            1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[0,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.loc[\"Alaska\":\"Arkansas\",\"2005\":\"2007\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  1\n",
      "0  iterative  1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[0:0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  1\n",
      "5  integrity  3\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[5:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  1\n",
      "0  iterative  1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[:1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  1\n",
      "3   existing  1\n",
      "4      query  4\n",
      "5  integrity  3\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[3:6][0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2      results\n",
      "3     existing\n",
      "4        query\n",
      "5    integrity\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[0][2:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0  1\n",
      "0     iterative  1\n",
      "1    similarity  1\n",
      "2       results  3\n",
      "3      existing  1\n",
      "4         query  4\n",
      "5     integrity  3\n",
      "6     reasoning  1\n",
      "7  consolidates  1\n",
      "8     concerned  1\n",
      "9   equivalence  1\n"
     ]
    }
   ],
   "source": [
    "print(wordlist[:-2][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0  1\n",
      "0     iterative  1\n",
      "1    similarity  1\n",
      "2       results  3\n",
      "3      existing  1\n",
      "4         query  4\n",
      "..          ... ..\n",
      "461        beds  1\n",
      "462        2003  1\n",
      "463        2000  1\n",
      "464        time  1\n",
      "465  validation  1\n",
      "\n",
      "[466 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[:,:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(wordlist.loc[0:0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(wordlist.loc[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>of</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>and</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>a</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>vocabulary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>weaklystructured</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>widespread</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>within</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>writing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0   1\n",
       "22                the  52\n",
       "396                of  39\n",
       "204               and  33\n",
       "109                 a  28\n",
       "11                 to  26\n",
       "..                ...  ..\n",
       "455        vocabulary   1\n",
       "145  weaklystructured   1\n",
       "98         widespread   1\n",
       "406            within   1\n",
       "120           writing   1\n",
       "\n",
       "[466 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.sort_values('2')\n",
    "df.sort_values(df.columns[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "sortwordlist = wordlist.sort_values(by=[1,0], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist.loc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0   1\n",
      "22                the  52\n",
      "396                of  39\n",
      "204               and  33\n",
      "109                 a  28\n",
      "11                 to  26\n",
      "..                ...  ..\n",
      "455        vocabulary   1\n",
      "145  weaklystructured   1\n",
      "98         widespread   1\n",
      "406            within   1\n",
      "120           writing   1\n",
      "\n",
      "[466 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    0   1\n",
      "0                 the  52\n",
      "1                  of  39\n",
      "2                 and  33\n",
      "3                   a  28\n",
      "4                  to  26\n",
      "..                ...  ..\n",
      "461        vocabulary   1\n",
      "462  weaklystructured   1\n",
      "463        widespread   1\n",
      "464            within   1\n",
      "465           writing   1\n",
      "\n",
      "[466 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "sortwordlist = sortwordlist.reset_index(drop=True)\n",
    "print(sortwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.65\n"
     ]
    }
   ],
   "source": [
    "print(totwords15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wordlist.loc[4][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "print(sortwordlist.loc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "the\n",
      "39\n",
      "of\n",
      "33\n",
      "and\n",
      "28\n",
      "a\n",
      "152\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "i = 0\n",
    "\n",
    "while sum < totwords15:\n",
    "    sum += sortwordlist.loc[i][1]\n",
    "    print(sortwordlist.loc[i][1])\n",
    "    print(sortwordlist.loc[i][0])\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print(sum)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 52\n",
      "of 39\n",
      "and 33\n",
      "a 28\n",
      "152\n",
      "4\n",
      "The minimum number of unique words to account for 15% fo the total number of words in the collection is 4 .\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "i = 0\n",
    "\n",
    "while sum < totwords15:\n",
    "    sum += sortwordlist.loc[i][1]\n",
    "    print(sortwordlist.loc[i][0], sortwordlist.loc[i][1])\n",
    "    i += 1\n",
    "\n",
    "print(sum)\n",
    "print(i)\n",
    "print('The minimum number of unique words to account for 15% fo the total number of words in the collection is', i,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #make it like an Excel worksheet\n",
    "# sortwordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "# #pd.set_option('display.max_rows', 1000)\n",
    "# pd.set_option('display.max_rows', 10)\n",
    "sortwordlist = wordlist.sort_values(by=[1,0], ascending=[False, True])\n",
    "sortwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #make it like an Excel worksheet\n",
    "# sortwordlist = pd.DataFrame(freq_word)\n",
    "\n",
    "# #pd.set_option('display.max_rows', 1000)\n",
    "# pd.set_option('display.max_rows', 10)\n",
    "sortwordlist = wordlist.sort_values(by=[1,0], ascending=[False, True])\n",
    "#sortwordlist = sortwordlist.rename(columns={0: 'word', 1: 'freq'})\n",
    "sortwordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sortwordlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sortwordlist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sortwordlist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sortwordlist[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(wordlist, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = sorted[wordlist, key=itemgetter(1), reverse=True]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array(wordlist)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### b. Clean Data Further:\n",
    "### Stemming/Lemmatization, Stopwords and Shortwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "#get stemming words or lemmas\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemlemstop_tokens = (wordnet_lemmatizer.lemmatize(word) for word in tokens)\n",
    "stemlemstop_tokens\n",
    "\n",
    "# remove common words\n",
    "stoplist = stopwords.words('english')\n",
    "# if you want to remove additional words\n",
    "#more = set(['much','even','time','story','character','from','went','saw','movie','last','night','see','knew','films','film','one','one','would','also','seen','watch','dvd','get','bit','movies','two','three','whose'])\n",
    "#more = set(['the'])\n",
    "#stoplist = set(stoplist) | more\n",
    "stoplist = set(stoplist)\n",
    "\n",
    "stemlemstop_tokens = [[word for word in text if word not in stoplist] for text in tokens]\n",
    "\n",
    "# remove short words\n",
    "stemlemstop_tokens = [[word for word in tokens if len(word) >= 3 ] for tokens in stemlemstop_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove common words\n",
    "stoplist = stopwords.words('english')\n",
    "# if you want to remove additional words\n",
    "#more = set(['much','even','time','story','character','from','went','saw','movie','last','night','see','knew','films','film','one','one','would','also','seen','watch','dvd','get','bit','movies','two','three','whose'])\n",
    "#more = set(['the'])\n",
    "#stoplist = set(stoplist) | more\n",
    "stoplist = set(stoplist)\n",
    "stemmed_stop_tokens = [word for word in stemmed_tokens if word not in stoplist]\n",
    "print(stemmed_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(stemmed_stop_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter non-alphanumeric characters from tokens\n",
    "stemmed_stop_tokens = [word for word in stemmed_stop_tokens if word.isalpha()]\n",
    "print(len(stemmed_stop_tokens))\n",
    "print(stemmed_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove short words\n",
    "stemmed_stop_tokens = [word for word in stemmed_stop_tokens if len(word) >= 3]\n",
    "print(len(stemmed_stop_tokens))\n",
    "print(stemmed_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_stemmed_stop_tokens = nltk.FreqDist(stemmed_stop_tokens)\n",
    "fdist_stemmed_stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_stemmed_stop_tokens.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print 1st review in list pos_rev_stopshort_texts\n",
    "print(stemmed_stop_tokens[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(stemmed_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count words in review types after cleaning\n",
    "\n",
    "#clean_stemmed_stop_tokens = [x for y in stemmed_stop_tokens for x in y]\n",
    "\n",
    "stemmed_stop_tokens_wordcounts = Counter(stemmed_stop_tokens)\n",
    "#neg_rev_stopshort_texts_wordcounts = Counter(clean_neg_rev_stopshort_tokens)\n",
    "#neu_rev_stopshort_texts_wordcounts = Counter(clean_neu_rev_stopshort_tokens)\n",
    "\n",
    "#print(len(neg_rev_stopshort_texts_wordcounts))\n",
    "#print(len(neu_rev_stopshort_texts_wordcounts))\n",
    "\n",
    "print(len(stemmed_stop_tokens_wordcounts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(stemmed_stop_tokens_wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print 1st 20 words in list clean_stemlemstop_tokens\n",
    "print(stemmed_stop_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist_stemmed_stop_tokens = nltk.FreqDist(stemmed_stop_tokens_wordcounts)\n",
    "fdist_stemmed_stop_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_stemmed_stop_tokens.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#get stemming words or lemmas\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#stemlem_tokens = (wordnet_lemmatizer.lemmatize(word) for word in tokens)\n",
    "stemlem_tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# for a in stemlem_tokens:\n",
    "#     print(a)\n",
    "    \n",
    "print(stemlem_tokens)\n",
    "#stemlem_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stemlemstop_tokens = [[word for word in text if word not in stoplist] for text in tokens]\n",
    "\n",
    "# remove short words\n",
    "stemlemstop_tokens = [[word for word in tokens if len(word) >= 3 ] for tokens in stemlemstop_tokens]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(stemlemstop_tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(stemlemstop_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(stemlemstop_tokens))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st review in list pos_rev_stopshort_texts\n",
    "print(stemlemstop_tokens[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# count words in review types after cleaning\n",
    "\n",
    "clean_stemlemstop_tokens = [x for y in stemlemstop_tokens for x in y]\n",
    "\n",
    "clean_stemlemstop_tokens_wordcounts = Counter(clean_stemlemstop_tokens)\n",
    "#neg_rev_stopshort_texts_wordcounts = Counter(clean_neg_rev_stopshort_tokens)\n",
    "#neu_rev_stopshort_texts_wordcounts = Counter(clean_neu_rev_stopshort_tokens)\n",
    "\n",
    "#print(len(neg_rev_stopshort_texts_wordcounts))\n",
    "#print(len(neu_rev_stopshort_texts_wordcounts))\n",
    "\n",
    "print(len(clean_stemlemstop_tokens_wordcounts))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print 1st 20 words in list clean_stemlemstop_tokens\n",
    "print(clean_stemlemstop_tokens[:20])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print top 20 words in sms_data_stopshort_texts_wordcounts with count\n",
    "print(sms_data_stopshort_texts_wordcounts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
